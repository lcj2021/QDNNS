{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 False\n",
      "data_path: /data/disk1/liuchengjun/HNNS/sample/wikipedia.base.M_32.efc_1000.efs_1000.ck_ts_1000.ncheck_100.recall@1000\n",
      "3415 175820\n"
     ]
    }
   ],
   "source": [
    "# conda activate /home/zhengweiguo/miniconda3/envs/hnns\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from binary_io import *\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "argparse = argparse.ArgumentParser()\n",
    "\n",
    "dataset = 'gist1m'\n",
    "dataset = 'deep100m.base'\n",
    "dataset = 'datacomp-image.base'\n",
    "dataset = 'imagenet.base'\n",
    "dataset = 'datacomp-text.base'\n",
    "dataset = 'spacev100m.base'\n",
    "dataset = 'wikipedia.base'\n",
    "config = json.loads(open('config.json').read())\n",
    "M, efs = config[dataset][\"M\"], config[dataset][\"efs\"]\n",
    "\n",
    "threshold = 950\n",
    "query_only = False\n",
    "print(threshold, query_only)\n",
    "dim = config[dataset][\"dim\"]\n",
    "efc = 1000\n",
    "ck_ts = 1000\n",
    "k = 1000\n",
    "topk = 0\n",
    "\n",
    "data_prefix = '/data/disk1/liuchengjun/HNNS/sample/'\n",
    "checkpoint_prefix = '/data/disk1/liuchengjun/HNNS/checkpoint/'\n",
    "prefix = f'{dataset}.M_{M}.efc_{efc}.efs_{efs}.ck_ts_{ck_ts}.ncheck_100.recall@{k}'\n",
    "\n",
    "train_feature = fvecs_read(f'{data_prefix}{prefix}.train_feats_nn.fvecs')\n",
    "test_feature = fvecs_read(f'{data_prefix}{prefix}.test_feats_nn.fvecs')[:, :]\n",
    "if query_only:\n",
    "    train_feature = train_feature[:, :dim]\n",
    "    test_feature = test_feature[:, :dim]\n",
    "\n",
    "print(f'data_path: {data_prefix}{prefix}')\n",
    "train_number_recall = ivecs_read(f'{data_prefix}{prefix}.train_label.ivecs')[:, 0]\n",
    "train_comps = ivecs_read(f'{data_prefix}{prefix}.train_label.ivecs')[:, 1]\n",
    "test_number_recall = ivecs_read(f'{data_prefix}{prefix}.test_label.ivecs')[:, 0]\n",
    "test_comps = ivecs_read(f'{data_prefix}{prefix}.test_label.ivecs')[:, 1]\n",
    "\n",
    "train_label_classification = train_number_recall < threshold\n",
    "test_label_classification = test_number_recall < threshold\n",
    "# train_label_classification = 1.000 - train_number_recall / 1000\n",
    "# test_label_classification = 1.000 - test_number_recall / 1000\n",
    "train_label_regression = np.log2(train_comps + 1)\n",
    "test_label_regression = np.log2(test_comps + 1)\n",
    "\n",
    "print(np.sum(test_label_classification), np.sum(train_label_classification))\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "offset = 0\n",
    "feat_query = train_feature[:, offset: offset + dim]\n",
    "query_cols = [f\"query_{i}\" for i in range(dim)]\n",
    "df_query = pd.DataFrame(feat_query, columns=query_cols)\n",
    "df = pd.concat([df, df_query], axis = 1)\n",
    "offset += dim\n",
    "\n",
    "feat_query = train_feature[:, offset: offset + topk * (topk - 1) // 2]\n",
    "query_cols = [f\"cross_{i}\" for i in range(topk * (topk - 1) // 2)]\n",
    "df_query = pd.DataFrame(feat_query, columns=query_cols)\n",
    "df = pd.concat([df, df_query], axis = 1)\n",
    "offset += topk * (topk - 1) // 2\n",
    "\n",
    "if offset < len(train_feature[0]):\n",
    "    feat_dist = train_feature[:, offset: offset + 100]\n",
    "    dist_cols = [f\"dist_{i}\" for i in range(100)]\n",
    "    df_dist = pd.DataFrame(feat_dist, columns=dist_cols)\n",
    "    df = pd.concat([df, df_dist], axis = 1)\n",
    "    offset += 100\n",
    "if offset < len(train_feature[0]):\n",
    "    feat_dist = train_feature[:, offset: offset + 10]\n",
    "    degree_cols = [f\"degree_{i}\" for i in range(10)]\n",
    "    df_degree = pd.DataFrame(feat_dist, columns=degree_cols)\n",
    "    df = pd.concat([df, df_degree], axis = 1)\n",
    "    offset += 10\n",
    "if offset < len(train_feature[0]):\n",
    "    feat_update = train_feature[:, offset: offset + 3]\n",
    "    update_cols = [f\"update_{i}\" for i in range(3)]\n",
    "    df_update = pd.DataFrame(feat_update, columns=update_cols)\n",
    "    df = pd.concat([df, df_update], axis = 1)\n",
    "    offset += 3\n",
    "\n",
    "assert offset == len(train_feature[0])\n",
    "\n",
    "df_number_recall = pd.DataFrame(train_number_recall.reshape(-1, 1), columns=[\"number_recall\"])\n",
    "df_comps = pd.DataFrame(train_comps.reshape(-1, 1), columns=[\"comps\"])\n",
    "df = pd.concat([df, df_number_recall, df_comps], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    500000.000000\n",
      "mean          8.975452\n",
      "std           3.121855\n",
      "min           0.000000\n",
      "25%           7.000000\n",
      "50%           9.000000\n",
      "75%          11.000000\n",
      "max          28.000000\n",
      "Name: update_0, dtype: float64\n",
      "count    500000.000000\n",
      "mean         23.260653\n",
      "std          25.144327\n",
      "min           0.000000\n",
      "25%           5.000000\n",
      "50%          15.000000\n",
      "75%          32.000000\n",
      "max         247.000000\n",
      "Name: update_1, dtype: float64\n",
      "count    500000.000000\n",
      "mean        214.972977\n",
      "std          54.556202\n",
      "min          66.000000\n",
      "25%         176.000000\n",
      "50%         211.000000\n",
      "75%         250.000000\n",
      "max         525.000000\n",
      "Name: update_2, dtype: float64\n",
      "根据update_0分成5个桶，每个桶的number_recall的最小值和最大值情况如下：\n",
      "桶0：最小值为820.0，最大值为996.0, 中位数为955.0, 左边界为0.0, 右边界为5.0\n",
      "桶1：最小值为848.0，最大值为998.0, 中位数为966.0, 左边界为6.0, 右边界为11.0\n",
      "桶2：最小值为882.0，最大值为999.0, 中位数为977.0, 左边界为12.0, 右边界为16.0\n",
      "桶3：最小值为905.0，最大值为999.0, 中位数为984.0, 左边界为17.0, 右边界为22.0\n",
      "桶4：最小值为936.4，最大值为999.3, 中位数为988.0, 左边界为23.0, 右边界为28.0\n",
      "根据update_1成5个桶，每个桶的number_recall的最小值和最大值情况如下：\n",
      "桶0：最小值为846.0，最大值为998.0, 中位数为966.0, 左边界为0.0, 右边界为49.0\n",
      "桶1：最小值为872.0，最大值为998.0, 中位数为974.0, 左边界为50.0, 右边界为98.0\n",
      "桶2：最小值为870.0，最大值为998.0, 中位数为972.0, 左边界为99.0, 右边界为148.0\n",
      "桶3：最小值为862.0，最大值为998.0, 中位数为967.0, 左边界为149.0, 右边界为197.0\n",
      "桶4：最小值为853.5，最大值为998.5, 中位数为964.0, 左边界为198.0, 右边界为247.0\n",
      "根据update_2成5个桶，每个桶的number_recall的最小值和最大值情况如下：\n",
      "桶0：最小值为820.0，最大值为996.0, 中位数为954.0, 左边界为66.0, 右边界为157.0\n",
      "桶1：最小值为848.0，最大值为998.0, 中位数为966.0, 左边界为158.0, 右边界为249.0\n",
      "桶2：最小值为878.0，最大值为998.0, 中位数为976.0, 左边界为250.0, 右边界为341.0\n",
      "桶3：最小值为898.0，最大值为999.0, 中位数为981.0, 左边界为342.0, 右边界为433.0\n",
      "桶4：最小值为885.4，最大值为999.0, 中位数为979.0, 左边界为434.0, 右边界为525.0\n"
     ]
    }
   ],
   "source": [
    "print(df['update_0'].describe())\n",
    "print(df['update_1'].describe())\n",
    "print(df['update_2'].describe())\n",
    "\n",
    "df['update_0_bin'] = pd.cut(df['update_0'], bins=5, labels=False)\n",
    "df['update_1_bin'] = pd.cut(df['update_1'], bins=5, labels=False)\n",
    "df['update_2_bin'] = pd.cut(df['update_2'], bins=5, labels=False)\n",
    "\n",
    "def get_min_max(df, column, target_column='number_recall'):\n",
    "    # comps\n",
    "    result_dict = {}\n",
    "    for bucket in range(5):\n",
    "        subset = df[df[column] == bucket][target_column]\n",
    "        if subset.empty:\n",
    "            result_dict[bucket] = (None, None)\n",
    "        else:\n",
    "            min_value = np.percentile(subset, 5)\n",
    "            max_value = np.percentile(subset, 95)\n",
    "            median = subset.median()\n",
    "            # 还需要存每个桶的左右边界\n",
    "            lb = df[df[column] == bucket][column[:-4]].min()\n",
    "            rb = df[df[column] == bucket][column[:-4]].max()\n",
    "            result_dict[bucket] = (min_value, max_value, median, lb, rb)\n",
    "    return result_dict\n",
    "\n",
    "result_dict_update_0 = get_min_max(df, 'update_0_bin')\n",
    "result_dict_update_1 = get_min_max(df, 'update_1_bin')\n",
    "result_dict_update_2 = get_min_max(df, 'update_2_bin')\n",
    "\n",
    "print(\"根据update_0分成5个桶，每个桶的number_recall的最小值和最大值情况如下：\")\n",
    "for bucket, (min_val, max_val, median, lb, rb) in result_dict_update_0.items():\n",
    "    print(f\"桶{bucket}：最小值为{min_val}，最大值为{max_val}, 中位数为{median}, 左边界为{lb}, 右边界为{rb}\")\n",
    "\n",
    "print(\"根据update_1成5个桶，每个桶的number_recall的最小值和最大值情况如下：\")\n",
    "for bucket, (min_val, max_val, median, lb, rb) in result_dict_update_1.items():\n",
    "    print(f\"桶{bucket}：最小值为{min_val}，最大值为{max_val}, 中位数为{median}, 左边界为{lb}, 右边界为{rb}\")\n",
    "\n",
    "print(\"根据update_2成5个桶，每个桶的number_recall的最小值和最大值情况如下：\")\n",
    "for bucket, (min_val, max_val, median, lb, rb) in result_dict_update_2.items():\n",
    "    print(f\"桶{bucket}：最小值为{min_val}，最大值为{max_val}, 中位数为{median}, 左边界为{lb}, 右边界为{rb}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hnns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
